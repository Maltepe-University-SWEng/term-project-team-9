{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11658190,"sourceType":"datasetVersion","datasetId":7316042}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:56.145628Z","iopub.execute_input":"2025-05-08T08:27:56.146119Z","iopub.status.idle":"2025-05-08T08:27:56.190833Z","shell.execute_reply.started":"2025-05-08T08:27:56.146099Z","shell.execute_reply":"2025-05-08T08:27:56.190190Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/nasreddin/joke_tokenizer.model\n/kaggle/input/nasreddin/nasreddin_hoca_jokes.json\n/kaggle/input/nasreddin/cleaned_nasreddin_hoca.txt\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\nfrom datasets import load_dataset, Dataset\nimport pandas as pd\nimport torch\n\n# GPU kontrol√º\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:56.191991Z","iopub.execute_input":"2025-05-08T08:27:56.192277Z","iopub.status.idle":"2025-05-08T08:27:56.196472Z","shell.execute_reply.started":"2025-05-08T08:27:56.192261Z","shell.execute_reply":"2025-05-08T08:27:56.195799Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import json\nimport pandas as pd\nfrom datasets import Dataset\n\n# JSON dosyasƒ±nƒ± oku (full path ile)\nwith open(\"/kaggle/input/nasreddin/nasreddin_hoca_jokes.json\", \"r\", encoding=\"utf-8\") as f:\n    data = json.load(f)\n\n# Pandas DataFrame'e √ßevir\ndf = pd.DataFrame(data)\n\n# input + output'u birle≈ütir\ndf[\"text\"] = df[\"input\"] + \" \" + df[\"output\"]\n\n# HF dataset'e d√∂n√º≈üt√ºr\ndataset = Dataset.from_pandas(df[[\"text\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:56.197084Z","iopub.execute_input":"2025-05-08T08:27:56.197267Z","iopub.status.idle":"2025-05-08T08:27:56.227637Z","shell.execute_reply.started":"2025-05-08T08:27:56.197253Z","shell.execute_reply":"2025-05-08T08:27:56.227016Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# 1. Veri(g√ºncellemeye a√ßƒ±k h√ºcre)\ndf = pd.read_json(\"/kaggle/input/nasreddin/nasreddin_hoca_jokes.json\")\ndf[\"text\"] = df[\"input\"] + \" \" + df[\"output\"]\ndataset = Dataset.from_pandas(df[[\"text\"]])\n\n# 2. Tokenizer ve model\nmodel_name = \"ytu-ce-cosmos/turkish-gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n\n# 3. Tokenize\ndef tokenize(example):\n    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\ntokenized_dataset = dataset.map(tokenize, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:56.229177Z","iopub.execute_input":"2025-05-08T08:27:56.229456Z","iopub.status.idle":"2025-05-08T08:27:58.305078Z","shell.execute_reply.started":"2025-05-08T08:27:56.229437Z","shell.execute_reply":"2025-05-08T08:27:58.304379Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/225 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288ca42fb7834ec580f2ae61e25a3004"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"\n# 4. Eƒüitim arg√ºmanlarƒ±\n\ntraining_args = TrainingArguments(\n    output_dir=\"./joke_model\",\n    per_device_train_batch_size=2,  # Daha fazla veri tutabiliyorsa 2 yap\n    num_train_epochs=10,             # Daha fazla √∂ƒürenme\n    learning_rate=1e-5,             # Daha yumu≈üak √∂ƒürenme\n    warmup_steps=50,                # Isƒ±nma d√∂nemi\n    weight_decay=0.01,              # Regularization\n    logging_dir=\"./logs\",           # TensorBoard i√ßin\n    logging_steps=10,\n    save_steps=250,                 # Checkpoint aralƒ±ƒüƒ± artƒ±rƒ±ldƒ±\n    save_total_limit=2,     \n    report_to=\"none\",              # wandb vs istemiyorsun\n)\n\n\n\n# 5. Trainer\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator\n)\n\n# 6. Eƒüitimi ba≈ülat\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:27:58.305917Z","iopub.execute_input":"2025-05-08T08:27:58.306183Z","iopub.status.idle":"2025-05-08T08:29:22.314180Z","shell.execute_reply.started":"2025-05-08T08:27:58.306160Z","shell.execute_reply":"2025-05-08T08:29:22.313404Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/2124091181.py:21: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='570' max='570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [570/570 01:23, Epoch 10/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>2.105000</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.001900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.130800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.002400</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.835100</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.160000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.957000</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.712500</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>1.584600</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.688600</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.832300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.666300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.642400</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.720500</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.508300</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.492800</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.824500</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.652100</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.379800</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.567200</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.741000</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.404800</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>1.934500</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>1.280300</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>1.488500</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>1.336600</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>1.498700</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>1.409700</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>1.639600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.422300</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>1.438600</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>1.535400</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>1.501500</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>1.487400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>1.763900</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>1.344600</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>1.290200</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>1.561900</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>1.520000</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.472400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>1.316700</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>1.337800</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>1.336800</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>1.273500</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>1.378700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>1.618200</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>1.316900</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>1.515500</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>1.338000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.324800</td>\n    </tr>\n    <tr>\n      <td>510</td>\n      <td>1.445600</td>\n    </tr>\n    <tr>\n      <td>520</td>\n      <td>1.438000</td>\n    </tr>\n    <tr>\n      <td>530</td>\n      <td>1.586500</td>\n    </tr>\n    <tr>\n      <td>540</td>\n      <td>1.404000</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>1.383900</td>\n    </tr>\n    <tr>\n      <td>560</td>\n      <td>1.224000</td>\n    </tr>\n    <tr>\n      <td>570</td>\n      <td>1.452700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=570, training_loss=1.5653894876178942, metrics={'train_runtime': 83.4762, 'train_samples_per_second': 26.954, 'train_steps_per_second': 6.828, 'total_flos': 146976768000000.0, 'train_loss': 1.5653894876178942, 'epoch': 10.0})"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"from transformers import GPT2Tokenizer, GPT2LMHeadModel, pipeline\n\n# 1. Fine-tuned model yolunu belirt\nmodel_path = \"/kaggle/working/joke_model/checkpoint-570\"\n\n# 2. Tokenizer'ƒ± y√ºkle\ntokenizer = GPT2Tokenizer.from_pretrained(\"ytu-ce-cosmos/turkish-gpt2\")\n\n# 3. Fine-tuned modeli y√ºkle\nmodel = GPT2LMHeadModel.from_pretrained(model_path)\n\n# 4. √úretim pipeline'ƒ± olu≈ütur\ngenerator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n\n# 5. Prompt (ba≈ülangƒ±√ß ifadesi)\nprompt = \"Nasreddin Hoca'nƒ±n ba≈üƒ±na komik bir olay geldi. \"\n\n\n# 6. ≈ûaka √ºret\noutputs = generator(\n    prompt,\n    max_length=100,\n    num_return_sequences=1,\n    do_sample=True,\n    temperature=0.7,\n    top_k=40,\n    top_p=0.9,\n    repetition_penalty=1.5,\n    eos_token_id=tokenizer.eos_token_id,\n    pad_token_id=tokenizer.eos_token_id,\n    early_stopping=True,\n)\n\n# 7. Sonucu yazdƒ±r\nprint(\"üó£Ô∏è √úretilen Fƒ±kra:\\n\")\nprint(outputs[0][\"generated_text\"].replace(prompt, \"\").strip())\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}